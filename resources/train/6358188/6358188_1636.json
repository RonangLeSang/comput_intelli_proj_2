{"test_class": {"identifier": "DatasourceInputFormatTest", "superclass": "", "interfaces": "", "fields": [{"original_string": "@Rule\n  public final TemporaryFolder temporaryFolder = new TemporaryFolder();", "modifier": "@Rule\n  public final", "type": "TemporaryFolder", "declarator": "temporaryFolder = new TemporaryFolder()", "var_name": "temporaryFolder"}, {"original_string": "private List<WindowedDataSegment> segments1;", "modifier": "private", "type": "List<WindowedDataSegment>", "declarator": "segments1", "var_name": "segments1"}, {"original_string": "private List<WindowedDataSegment> segments2;", "modifier": "private", "type": "List<WindowedDataSegment>", "declarator": "segments2", "var_name": "segments2"}, {"original_string": "private List<LocatedFileStatus> locations;", "modifier": "private", "type": "List<LocatedFileStatus>", "declarator": "locations", "var_name": "locations"}, {"original_string": "private JobConf config;", "modifier": "private", "type": "JobConf", "declarator": "config", "var_name": "config"}, {"original_string": "private JobContext context;", "modifier": "private", "type": "JobContext", "declarator": "context", "var_name": "context"}, {"original_string": "private Supplier<InputFormat> testFormatter = new Supplier<InputFormat>()\n  {\n    @Override\n    public InputFormat get()\n    {\n      final Map<String, LocatedFileStatus> locationMap = new HashMap<>();\n      for (LocatedFileStatus status : locations) {\n        locationMap.put(status.getPath().getName(), status);\n      }\n\n      return new TextInputFormat()\n      {\n        @Override\n        protected boolean isSplitable(FileSystem fs, Path file)\n        {\n          return false;\n        }\n\n        @Override\n        protected FileStatus[] listStatus(JobConf job) throws IOException\n        {\n          Path[] dirs = getInputPaths(job);\n          if (dirs.length == 0) {\n            throw new IOException(\"No input paths specified in job\");\n          }\n          FileStatus[] status = new FileStatus[dirs.length];\n          for (int i = 0; i < dirs.length; i++) {\n            status[i] = locationMap.get(dirs[i].getName());\n          }\n          return status;\n        }\n      };\n    }\n  };", "modifier": "private", "type": "Supplier<InputFormat>", "declarator": "testFormatter = new Supplier<InputFormat>()\n  {\n    @Override\n    public InputFormat get()\n    {\n      final Map<String, LocatedFileStatus> locationMap = new HashMap<>();\n      for (LocatedFileStatus status : locations) {\n        locationMap.put(status.getPath().getName(), status);\n      }\n\n      return new TextInputFormat()\n      {\n        @Override\n        protected boolean isSplitable(FileSystem fs, Path file)\n        {\n          return false;\n        }\n\n        @Override\n        protected FileStatus[] listStatus(JobConf job) throws IOException\n        {\n          Path[] dirs = getInputPaths(job);\n          if (dirs.length == 0) {\n            throw new IOException(\"No input paths specified in job\");\n          }\n          FileStatus[] status = new FileStatus[dirs.length];\n          for (int i = 0; i < dirs.length; i++) {\n            status[i] = locationMap.get(dirs[i].getName());\n          }\n          return status;\n        }\n      };\n    }\n  }", "var_name": "testFormatter"}], "file": "indexing-hadoop/src/test/java/org/apache/druid/indexer/hadoop/DatasourceInputFormatTest.java"}, "test_case": {"identifier": "testGetRecordReader", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void testGetRecordReader()\n  {\n    Assert.assertTrue(new DatasourceInputFormat().createRecordReader(null, null) instanceof DatasourceRecordReader);\n  }", "signature": "void testGetRecordReader()", "full_signature": "@Test public void testGetRecordReader()", "class_method_signature": "DatasourceInputFormatTest.testGetRecordReader()", "testcase": true, "constructor": false, "invocations": ["assertTrue", "createRecordReader"]}, "focal_class": {"identifier": "DatasourceInputFormat", "superclass": "extends InputFormat<NullWritable, InputRow>", "interfaces": "", "fields": [{"original_string": "private static final Logger logger = new Logger(DatasourceInputFormat.class);", "modifier": "private static final", "type": "Logger", "declarator": "logger = new Logger(DatasourceInputFormat.class)", "var_name": "logger"}, {"original_string": "private static final String CONF_DATASOURCES = \"druid.datasource.input.datasources\";", "modifier": "private static final", "type": "String", "declarator": "CONF_DATASOURCES = \"druid.datasource.input.datasources\"", "var_name": "CONF_DATASOURCES"}, {"original_string": "private static final String CONF_SCHEMA = \"druid.datasource.input.schema\";", "modifier": "private static final", "type": "String", "declarator": "CONF_SCHEMA = \"druid.datasource.input.schema\"", "var_name": "CONF_SCHEMA"}, {"original_string": "private static final String CONF_SEGMENTS = \"druid.datasource.input.segments\";", "modifier": "private static final", "type": "String", "declarator": "CONF_SEGMENTS = \"druid.datasource.input.segments\"", "var_name": "CONF_SEGMENTS"}, {"original_string": "private static final String CONF_MAX_SPLIT_SIZE = \"druid.datasource.input.split.max.size\";", "modifier": "private static final", "type": "String", "declarator": "CONF_MAX_SPLIT_SIZE = \"druid.datasource.input.split.max.size\"", "var_name": "CONF_MAX_SPLIT_SIZE"}, {"original_string": "private Supplier<org.apache.hadoop.mapred.InputFormat> supplier = new Supplier<org.apache.hadoop.mapred.InputFormat>()\n  {\n    @Override\n    public org.apache.hadoop.mapred.InputFormat get()\n    {\n      return new TextInputFormat()\n      {\n        //Always consider non-splittable as we only want to get location of blocks for the segment\n        //and not consider the splitting.\n        //also without this, isSplitable(..) fails with NPE because compressionCodecs is not properly setup.\n        @Override\n        protected boolean isSplitable(FileSystem fs, Path file)\n        {\n          return false;\n        }\n\n        @Override\n        protected FileStatus[] listStatus(JobConf job) throws IOException\n        {\n          // to avoid globbing which needs input path should be hadoop-compatible (':' is not acceptable in path, etc.)\n          List<FileStatus> statusList = new ArrayList<>();\n          for (Path path : FileInputFormat.getInputPaths(job)) {\n            // load spec in segment points specifically zip file itself\n            statusList.add(path.getFileSystem(job).getFileStatus(path));\n          }\n          return statusList.toArray(new FileStatus[0]);\n        }\n      };\n    }\n  };", "modifier": "private", "type": "Supplier<org.apache.hadoop.mapred.InputFormat>", "declarator": "supplier = new Supplier<org.apache.hadoop.mapred.InputFormat>()\n  {\n    @Override\n    public org.apache.hadoop.mapred.InputFormat get()\n    {\n      return new TextInputFormat()\n      {\n        //Always consider non-splittable as we only want to get location of blocks for the segment\n        //and not consider the splitting.\n        //also without this, isSplitable(..) fails with NPE because compressionCodecs is not properly setup.\n        @Override\n        protected boolean isSplitable(FileSystem fs, Path file)\n        {\n          return false;\n        }\n\n        @Override\n        protected FileStatus[] listStatus(JobConf job) throws IOException\n        {\n          // to avoid globbing which needs input path should be hadoop-compatible (':' is not acceptable in path, etc.)\n          List<FileStatus> statusList = new ArrayList<>();\n          for (Path path : FileInputFormat.getInputPaths(job)) {\n            // load spec in segment points specifically zip file itself\n            statusList.add(path.getFileSystem(job).getFileStatus(path));\n          }\n          return statusList.toArray(new FileStatus[0]);\n        }\n      };\n    }\n  }", "var_name": "supplier"}], "methods": [{"identifier": "getSplits", "parameters": "(JobContext context)", "modifiers": "@Override public", "return": "List<InputSplit>", "signature": "List<InputSplit> getSplits(JobContext context)", "full_signature": "@Override public List<InputSplit> getSplits(JobContext context)", "class_method_signature": "DatasourceInputFormat.getSplits(JobContext context)", "testcase": false, "constructor": false}, {"identifier": "createRecordReader", "parameters": "(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "modifiers": "@Override public", "return": "RecordReader<NullWritable, InputRow>", "signature": "RecordReader<NullWritable, InputRow> createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "full_signature": "@Override public RecordReader<NullWritable, InputRow> createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "class_method_signature": "DatasourceInputFormat.createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "testcase": false, "constructor": false}, {"identifier": "setSupplier", "parameters": "(Supplier<org.apache.hadoop.mapred.InputFormat> supplier)", "modifiers": "@VisibleForTesting", "return": "DatasourceInputFormat", "signature": "DatasourceInputFormat setSupplier(Supplier<org.apache.hadoop.mapred.InputFormat> supplier)", "full_signature": "@VisibleForTesting DatasourceInputFormat setSupplier(Supplier<org.apache.hadoop.mapred.InputFormat> supplier)", "class_method_signature": "DatasourceInputFormat.setSupplier(Supplier<org.apache.hadoop.mapred.InputFormat> supplier)", "testcase": false, "constructor": false}, {"identifier": "toDataSourceSplit", "parameters": "(\n      List<WindowedDataSegment> segments,\n      org.apache.hadoop.mapred.InputFormat fio,\n      JobConf conf\n  )", "modifiers": "private", "return": "DatasourceInputSplit", "signature": "DatasourceInputSplit toDataSourceSplit(\n      List<WindowedDataSegment> segments,\n      org.apache.hadoop.mapred.InputFormat fio,\n      JobConf conf\n  )", "full_signature": "private DatasourceInputSplit toDataSourceSplit(\n      List<WindowedDataSegment> segments,\n      org.apache.hadoop.mapred.InputFormat fio,\n      JobConf conf\n  )", "class_method_signature": "DatasourceInputFormat.toDataSourceSplit(\n      List<WindowedDataSegment> segments,\n      org.apache.hadoop.mapred.InputFormat fio,\n      JobConf conf\n  )", "testcase": false, "constructor": false}, {"identifier": "getLocations", "parameters": "(\n      final List<WindowedDataSegment> segments,\n      final org.apache.hadoop.mapred.InputFormat fio,\n      final JobConf conf\n  )", "modifiers": "@VisibleForTesting static", "return": "Stream<String>", "signature": "Stream<String> getLocations(\n      final List<WindowedDataSegment> segments,\n      final org.apache.hadoop.mapred.InputFormat fio,\n      final JobConf conf\n  )", "full_signature": "@VisibleForTesting static Stream<String> getLocations(\n      final List<WindowedDataSegment> segments,\n      final org.apache.hadoop.mapred.InputFormat fio,\n      final JobConf conf\n  )", "class_method_signature": "DatasourceInputFormat.getLocations(\n      final List<WindowedDataSegment> segments,\n      final org.apache.hadoop.mapred.InputFormat fio,\n      final JobConf conf\n  )", "testcase": false, "constructor": false}, {"identifier": "getFrequentLocations", "parameters": "(final Stream<String> locations)", "modifiers": "@VisibleForTesting static", "return": "String[]", "signature": "String[] getFrequentLocations(final Stream<String> locations)", "full_signature": "@VisibleForTesting static String[] getFrequentLocations(final Stream<String> locations)", "class_method_signature": "DatasourceInputFormat.getFrequentLocations(final Stream<String> locations)", "testcase": false, "constructor": false}, {"identifier": "getDataSources", "parameters": "(final Configuration conf)", "modifiers": "public static", "return": "List<String>", "signature": "List<String> getDataSources(final Configuration conf)", "full_signature": "public static List<String> getDataSources(final Configuration conf)", "class_method_signature": "DatasourceInputFormat.getDataSources(final Configuration conf)", "testcase": false, "constructor": false}, {"identifier": "getIngestionSpec", "parameters": "(final Configuration conf, final String dataSource)", "modifiers": "public static", "return": "DatasourceIngestionSpec", "signature": "DatasourceIngestionSpec getIngestionSpec(final Configuration conf, final String dataSource)", "full_signature": "public static DatasourceIngestionSpec getIngestionSpec(final Configuration conf, final String dataSource)", "class_method_signature": "DatasourceInputFormat.getIngestionSpec(final Configuration conf, final String dataSource)", "testcase": false, "constructor": false}, {"identifier": "getSegments", "parameters": "(final Configuration conf, final String dataSource)", "modifiers": "public static", "return": "List<WindowedDataSegment>", "signature": "List<WindowedDataSegment> getSegments(final Configuration conf, final String dataSource)", "full_signature": "public static List<WindowedDataSegment> getSegments(final Configuration conf, final String dataSource)", "class_method_signature": "DatasourceInputFormat.getSegments(final Configuration conf, final String dataSource)", "testcase": false, "constructor": false}, {"identifier": "getMaxSplitSize", "parameters": "(final Configuration conf, final String dataSource)", "modifiers": "public static", "return": "long", "signature": "long getMaxSplitSize(final Configuration conf, final String dataSource)", "full_signature": "public static long getMaxSplitSize(final Configuration conf, final String dataSource)", "class_method_signature": "DatasourceInputFormat.getMaxSplitSize(final Configuration conf, final String dataSource)", "testcase": false, "constructor": false}, {"identifier": "addDataSource", "parameters": "(\n      final Configuration conf,\n      final DatasourceIngestionSpec spec,\n      final List<WindowedDataSegment> segments,\n      final long maxSplitSize\n  )", "modifiers": "public static", "return": "void", "signature": "void addDataSource(\n      final Configuration conf,\n      final DatasourceIngestionSpec spec,\n      final List<WindowedDataSegment> segments,\n      final long maxSplitSize\n  )", "full_signature": "public static void addDataSource(\n      final Configuration conf,\n      final DatasourceIngestionSpec spec,\n      final List<WindowedDataSegment> segments,\n      final long maxSplitSize\n  )", "class_method_signature": "DatasourceInputFormat.addDataSource(\n      final Configuration conf,\n      final DatasourceIngestionSpec spec,\n      final List<WindowedDataSegment> segments,\n      final long maxSplitSize\n  )", "testcase": false, "constructor": false}], "file": "indexing-hadoop/src/main/java/org/apache/druid/indexer/hadoop/DatasourceInputFormat.java"}, "focal_method": {"identifier": "createRecordReader", "parameters": "(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "modifiers": "@Override public", "return": "RecordReader<NullWritable, InputRow>", "body": "@Override\n  public RecordReader<NullWritable, InputRow> createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )\n  {\n    return new DatasourceRecordReader();\n  }", "signature": "RecordReader<NullWritable, InputRow> createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "full_signature": "@Override public RecordReader<NullWritable, InputRow> createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "class_method_signature": "DatasourceInputFormat.createRecordReader(\n      InputSplit split,\n      TaskAttemptContext context\n  )", "testcase": false, "constructor": false, "invocations": []}, "repository": {"repo_id": 6358188, "url": "https://github.com/apache/druid", "stars": 9116, "created": "10/23/2012 7:08:07 PM +00:00", "updates": "2020-01-27T21:36:20+00:00", "fork": "False", "license": "licensed"}}